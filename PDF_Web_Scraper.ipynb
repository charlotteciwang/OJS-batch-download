{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# README: IMPORTANT!!!\n",
    "# CREATE A FOLDER CALLED 'downloaded_pdfs' in the same folder as you put this script\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import urllib\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list for all doi's to scrape\n",
    "doi_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.analyticsvidhya.com/blog/2021/08/python-tutorial-working-with-csv-file-for-data-science/\n",
    "with open('example.csv') as file:\n",
    "    csvreader = csv.reader(file)\n",
    "    for [row] in csvreader:\n",
    "        doi_list.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3154"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the number of URLs in the csv file\n",
    "len(doi_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://doi.org/10.7916/jmetc.v4i2.626 downloaded\n"
     ]
    }
   ],
   "source": [
    "# IF statements are made to fit different websites in our doi list\n",
    "\n",
    "# start scraping\n",
    "#for i in range(1737,1740): # for testing\n",
    "for i in range(len(doi_list)):\n",
    "    type1_list = ['alusur', 'jla', 'cjal', 'cjgl', 'consilience']\n",
    "    for subtype in type1_list:\n",
    "        if subtype in doi_list[i]:\n",
    "            page = requests.get(doi_list[i])\n",
    "            soup = bs(page.content, 'html.parser')\n",
    "\n",
    "            # get the inspect element from the doi page\n",
    "            tag_to_pdf = soup.find('a', class_ = 'btn btn-primary')\n",
    "            \n",
    "            # record the doi if the pdf file is missing on the website\n",
    "            if tag_to_pdf is None:\n",
    "                with open('missing.txt', 'a+') as f:\n",
    "                    f.write(doi_list[i] + '\\n')\n",
    "                print(doi_list[i] + ' has no pdf')\n",
    "            \n",
    "            else:\n",
    "                # get the url that leads to download\n",
    "                link_to_pdf = tag_to_pdf['href']\n",
    "                \n",
    "                # use link_to_pdf to download the pdf\n",
    "                page = requests.get(link_to_pdf)\n",
    "                soup = bs(page.content, 'html.parser')\n",
    "                \n",
    "                # https://stackoverflow.com/questions/22726860/beautifulsoup-webscraping-find-all-finding-exact-match\n",
    "                tag_to_pdf = soup.find(lambda tag: tag.name == 'a' and tag.get('class') == ['btn']) \n",
    "                download_link = tag_to_pdf['href']\n",
    "                \n",
    "                # save as pdf\n",
    "                # https://stackoverflow.com/questions/19056031/download-files-using-requests-and-beautifulsoup\n",
    "                newfilename = link_to_pdf.split('/')[-2] + link_to_pdf.split('/')[-1] + '.pdf'\n",
    "                response = requests.get(download_link)\n",
    "                open('downloaded_pdfs/'+newfilename, \"wb\").write(response.content)\n",
    "                print(doi_list[i] + ' downloaded')\n",
    "    \n",
    "    type2_list = ['btpp', 'cjrl', 'cblr','cswr', 'jcept', 'jmetc', 'salt', 'vib', 'thejgh', 'waxingmoon']\n",
    "    for subtype in type2_list:\n",
    "        if subtype in doi_list[i]:\n",
    "            page = requests.get(doi_list[i])\n",
    "            soup = bs(page.content, 'html.parser')\n",
    "\n",
    "            # get the inspect element from the doi page\n",
    "            tag_to_pdf = soup.find('a', class_ = 'galley-link')\n",
    "            \n",
    "            # record the doi if the pdf file is missing on the website\n",
    "            if tag_to_pdf is None:\n",
    "                print('true')\n",
    "                with open('missing.txt', 'a+') as f:\n",
    "                    f.write(doi_list[i] + '\\n')\n",
    "                print(doi_list[i] + ' has no pdf')\n",
    "            \n",
    "            else:\n",
    "                # get the url that leads to download\n",
    "                link_to_pdf = tag_to_pdf['href']\n",
    "\n",
    "                # use link_to_pdf to download the pdf\n",
    "                page = requests.get(link_to_pdf)\n",
    "                soup = bs(page.content, 'html.parser')\n",
    "\n",
    "                tag_to_pdf = soup.find('a', {'class':'download'})\n",
    "                download_link = tag_to_pdf['href']\n",
    "                \n",
    "                # save as pdf\n",
    "                # https://stackoverflow.com/questions/19056031/download-files-using-requests-and-beautifulsoup\n",
    "                newfilename = link_to_pdf.split('/')[-2] + link_to_pdf.split('/')[-1] + '.pdf'\n",
    "                response = requests.get(download_link)\n",
    "                open('downloaded_pdfs/'+newfilename, \"wb\").write(response.content)\n",
    "                print(doi_list[i] + ' downloaded')\n",
    "    \n",
    "    type3_list = ['cjel', 'cjtl', 'cm', 'cusj', 'portales', 'stlr']\n",
    "    for subtype in type3_list:\n",
    "        if subtype in doi_list[i]:\n",
    "            page = requests.get(doi_list[i])\n",
    "            soup = bs(page.content, 'html.parser')\n",
    "\n",
    "            # get the inspect element from the doi page\n",
    "            tag_to_pdf = soup.find('a', class_ = 'btn btn-primary')\n",
    "\n",
    "            # record the doi if the pdf file is missing on the website\n",
    "            if tag_to_pdf is None:\n",
    "                with open('missing.txt', 'a+') as f:\n",
    "                    f.write(doi_list[i] + '\\n')\n",
    "                print(doi_list[i] + ' has no pdf')\n",
    "            \n",
    "            else:\n",
    "                # get the url that leads to download\n",
    "                link_to_pdf = tag_to_pdf['href']\n",
    "\n",
    "                # use link_to_pdf to download the pdf\n",
    "                page = requests.get(link_to_pdf)\n",
    "                soup = bs(page.content, 'html.parser')\n",
    "                # https://stackoverflow.com/questions/22726860/beautifulsoup-webscraping-find-all-finding-exact-match\n",
    "                tag_to_pdf = soup.find(lambda tag: tag.name == 'a' and tag.get('class') == ['btn']) \n",
    "                download_link = tag_to_pdf['href']  \n",
    "                \n",
    "                # save as pdf\n",
    "                # https://stackoverflow.com/questions/19056031/download-files-using-requests-and-beautifulsoup\n",
    "                newfilename = link_to_pdf.split('/')[-2] + link_to_pdf.split('/')[-1] + '.pdf'\n",
    "                response = requests.get(download_link)\n",
    "                open('downloaded_pdfs/'+newfilename, \"wb\").write(response.content)\n",
    "                print(doi_list[i] + ' downloaded')\n",
    "    \n",
    "    # sleep for a few seconds\n",
    "    time.sleep(random.randint(5,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'<!DOCTYPE html>\\n<html lang=\"en-US\" xml:lang=\"en-US\">\\n<head>\\n\\t<meta charset=\"utf-8\">\\n\\t<meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\\n\\t<title>\\n\\t\\tView of Moving at a Glacial Pace: What Can State Attorneys General Do About SEC Inattention to Nondisclosure of Financially Material Risks Arising from Climate Change?\\n\\t\\t\\t\\t\\t\\t\\t| Columbia Journal of Environmental Law\\n\\t\\t\\t</title>\\n\\n\\t\\n<link rel=\"icon\" href=\"https://journals.library.columbia.edu/public/journals/24/favicon_en_US.png\">\\n<meta name=\"generator\" content=\"Open Journal Systems 3.3.0.10\">\\n\\n\\t<link rel=\"stylesheet\" href=\"https://journals.library.columbia.edu/plugins/themes/healthSciences/libs/bootstrap.min.css?v=3.3.0.10\" type=\"text/css\" /><link rel=\"stylesheet\" href=\"https://journals.library.columbia.edu/index.php/cjel/$$$call$$$/page/page/css?name=stylesheet\" type=\"text/css\" /><link rel=\"stylesheet\" href=\"https://journals.library.columbia.edu/index.php/cjel/$$$call$$$/page/page/css?name=custom\" type=\"text/css\" /><link rel=\"stylesheet\" href=\"https://journals.library.columbia.edu/plugins/generic/orcidProfile/css/orcidProfile.css?v=3.3.0.10\" type=\"text/css\" /><link rel=\"stylesheet\" href=\"https://journals.library.columbia.edu/public/journals/24/styleSheet.css?d=\" type=\"text/css\" />\\n</head>\\n<body class=\"page-view-pdf\">\\n\\t<div class=\"pdf-header\">\\n\\t\\t<div class=\"pdf-return-article\">\\n\\t\\t\\t<a href=\"https://journals.library.columbia.edu/index.php/cjel/article/view/3532\" class=\"btn btn-text\">\\n\\t\\t\\t\\t\\xe2\\x86\\x90\\n\\t\\t\\t\\t<span class=\"sr-only\">\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tReturn to Article Details\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t</span>\\n\\t\\t\\t\\t\\t\\t\\t\\t\\tMoving at a Glacial Pace: What Can State Attorneys General Do About SEC Inattention to Nondisclosure of Financially Material Risks Arising from Climate Change?\\n\\t\\t\\t\\t\\t\\t\\t</a>\\n\\t\\t</div>\\n\\t\\t\\t\\t<div class=\"pdf-download-button\">\\n\\t\\t\\t<a href=\"https://journals.library.columbia.edu/index.php/cjel/article/download/3532/1403/5997\" class=\"btn\" download>\\n\\t\\t\\t\\t<span class=\"label\">\\n\\t\\t\\t\\t\\tDownload\\n\\t\\t\\t\\t</span>\\n\\t\\t\\t</a>\\n\\t\\t</div>\\n\\t</div>\\n\\n\\t<div id=\"pdfCanvasContainer\" class=\"pdf-frame\">\\n\\t\\t<iframe src=\"https://journals.library.columbia.edu/plugins/generic/pdfJsViewer/pdf.js/web/viewer.html?file=https%3A%2F%2Fjournals.library.columbia.edu%2Findex.php%2Fcjel%2Farticle%2Fdownload%2F3532%2F1403%2F5997\" width=\"100%\" height=\"100%\" style=\"min-height: 500px;\" allowfullscreen webkitallowfullscreen></iframe>\\n\\t</div>\\n\\t\\n</body>\\n</html>\\n'"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# page.content"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
